{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aec8c668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install biobox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "6803f186",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import biobox as bb\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "03241443",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdb_2_biobox(pdb_file):\n",
    "    M = bb.Molecule()\n",
    "    M.import_pdb(pdb_file)\n",
    "    return M\n",
    "\n",
    "\n",
    "def extract_CA_coordinates(M):\n",
    "    ca_idx = (M.data['name']=='CA').values\n",
    "    ca_coords = M.coordinates[0][ca_idx]\n",
    "    \n",
    "    if ca_coords.shape[0] != M.data['resid'].nunique():\n",
    "        raise Exception(\"You better check your PDB... The number of CA atoms does not equal the number of ResIDs in your PDB file!\") \n",
    "    else:\n",
    "        return ca_coords\n",
    "\n",
    "    \n",
    "def extract_sequence(M):\n",
    "    \n",
    "    \n",
    "    aa_names = {\n",
    "                'A': 'ALA', 'C': 'CYS', 'D': 'ASP', 'E': 'GLU',\n",
    "                'F': 'PHE', 'G': 'GLY', 'H': 'HIS', 'I': 'ILE',\n",
    "                'K': 'LYS', 'L': 'LEU', 'M': 'MET', 'N': 'ASN',\n",
    "                'P': 'PRO', 'Q': 'GLN', 'R': 'ARG', 'S': 'SER',\n",
    "                'T': 'THR', 'V': 'VAL', 'W': 'TRP', 'Y': 'TYR'\n",
    "                }\n",
    "\n",
    "    names_aa = {y: x for x, y in aa_names.items()}\n",
    "    \n",
    "    ca_idx = (M.data['name']=='CA').values\n",
    "    resnames = M.data['resname'][ca_idx].map(names_aa).values\n",
    "    \n",
    "    if resnames.shape[0] != M.data['resid'].nunique():\n",
    "        raise Exception(\"You better check your PDB... The number of CA atoms does not equal the number of ResIDs in your PDB file!\") \n",
    "    else:\n",
    "        return resnames\n",
    "\n",
    "\n",
    "def write_fingerprint_file(number_chains, sequence, secondary_structure, working_path):\n",
    "    \n",
    "    assert isinstance(number_chains, int), 'Yikes... The number of chains is not int type!'\n",
    "    \n",
    "    if number_chains > 1:\n",
    "        print('Are sure you have more than one chain - if not this will cause segmentation errors later! You have been warned...')\n",
    "    \n",
    "    seq_run = ''.join(list(sequence))\n",
    "    ss_run = ''.join(list(secondary_structure))\n",
    "    \n",
    "    if len(seq_run) != len(ss_run):\n",
    "        raise Exception(\"Uh Oh... The length of sequence and secondary structure is not equal!\") \n",
    "    \n",
    "    f = open(working_path+\"/fingerPrint1.dat\", \"w\")\n",
    "    f.write(str(number_chains))\n",
    "    f.write('\\n \\n')\n",
    "    f.write(seq_run)\n",
    "    f.write('\\n \\n')\n",
    "    f.write(ss_run)\n",
    "    f.close()\n",
    "    \n",
    "    \n",
    "def write_coordinates_file(ca_coords, working_path):\n",
    "    \n",
    "    assert type(coords).__module__ == np.__name__, 'Thats never good... the CA coordinates are not a numpy array'\n",
    "    np.savetxt(working_path+'/coordinates1.dat', coords, delimiter=' ', fmt='%s',newline='\\n', header='', footer='')\n",
    "    \n",
    "    \n",
    "def write_mixture_file(working_path):\n",
    "    # if default:\n",
    "    f = open(working_path+\"/mixtureFile.dat\", \"w\")\n",
    "    f.write(str(1))\n",
    "        \n",
    "#     else:\n",
    "#          copy input file\n",
    "\n",
    "\n",
    "def write_varysections_file(varying_sections, working_path):\n",
    "    # auto: run beta sheet breaking code; write output sections to file\n",
    "    f = open(working_path+\"/varyingSectionSecondary1.dat\", \"w\")\n",
    "    for i, s in enumerate(varying_sections):\n",
    "        f.write(str(s))\n",
    "        \n",
    "        if i < len(varying_sections)-1:\n",
    "            f.write('\\n')\n",
    "    f.close()\n",
    "\n",
    "    \n",
    "def copy_saxs(SAXS_file, working_path):\n",
    "    \n",
    "    saxs_arr = np.genfromtxt(SAXS_file)\n",
    "    \n",
    "    if saxs_arr.shape[1] == 3:\n",
    "        saxs_arr = saxs_arr[:,:2]\n",
    "        \n",
    "    np.savetxt(working_path+'/Saxs.dat', saxs_arr, delimiter=' ', fmt='%s',newline='\\n', header='', footer='')\n",
    "\n",
    "\n",
    "def read_dssp_file(dssp_flname):\n",
    "    \n",
    "    simplify_dict = {'H': 'H', 'B': 'S', 'E': 'S', 'G': 'H', 'I': 'H', 'T': '-', 'S': '-', '-': '-', ' ': '-'}\n",
    "    \n",
    "    lines=[]\n",
    "    with open(dssp_flname) as input_data:\n",
    "        # Skips text before the beginning of the interesting block:\n",
    "        for line in input_data:\n",
    "            if line.strip() == '#  RESIDUE AA STRUCTURE BP1 BP2  ACC     N-H-->O    O-->H-N    N-H-->O    O-->H-N    TCO  KAPPA ALPHA  PHI   PSI    X-CA   Y-CA   Z-CA':  # Or whatever test is needed\n",
    "                break\n",
    "        # Reads text until the end of the block:\n",
    "        for line in input_data:  # This keeps reading the file\n",
    "            lines.append(simplify_dict[line[16]])\n",
    "    return ''.join(lines)\n",
    "    \n",
    "    \n",
    "def simplify_secondary(dssp_struct):\n",
    "    \n",
    "    simplify_dict = {'H': 'H', 'B': 'S', 'E': 'S', 'G': 'H', 'I': 'H', 'T': '-', 'S': '-', '-': '-', ' ': '-'}\n",
    "    \n",
    "    secondary_structure = []\n",
    "    \n",
    "    for s in dssp_struct:\n",
    "        \n",
    "        if s not in list(simplify_dict.keys()):\n",
    "            print('>>> ', s, ' <<<')\n",
    "            raise Exception('Secondary structure not recognised!')\n",
    "            \n",
    "        secondary_structure.append(simplify_dict[s])\n",
    "        \n",
    "    return secondary_structure\n",
    "\n",
    "\n",
    "def write_sh_file(working_path, fit_n_times, min_q, max_q, max_fit_steps):\n",
    "    curr = os.getcwd()\n",
    "    run_file = curr + '/RunMe.sh'\n",
    "    # output_filepath = os.path.basename(os.path.normpath(test_name))+'_'+project_name+'.sh'\n",
    "    print(run_file)\n",
    "    with open(run_file, 'w+') as fout:\n",
    "        fout.write('#!/bin/bash')\n",
    "        \n",
    "        saxs_file = working_path+'/Saxs.dat'\n",
    "        FP_file = working_path+\"/fingerPrint1.dat\"\n",
    "        coords_file = working_path+'/coordinates1.dat'\n",
    "        varying_file = working_path+\"/varyingSectionSecondary1.dat\"\n",
    "        mixture_file = working_path+\"/mixtureFile.dat\"\n",
    "    \n",
    "        # saxs_arr = np.genfromtxt(saxs_file)\n",
    "        # min_q = np.round(saxs_arr[:,0].min(),2)\n",
    "        # max_q = np.round(saxs_arr[:,0].max(),2)\n",
    "        \n",
    "        fout.write('\\nfor i in {1..'+str(fit_n_times)+'}')\n",
    "\n",
    "        fout.write('\\n\\ndo')\n",
    "        fout.write('\\n\\n   echo \" Run number : $i \"')\n",
    "        fout.write('\\n\\n   ./predictStructure ' + saxs_file + ' ' + working_path+'/' + ' ' + coords_file + ' ' + 'none' + ' ' + varying_file + ' ' + '1' + ' ' + 'none' + \\\n",
    "                   ' ' + 'none' + ' ' + str(min_q) + ' ' + str(max_q) + ' ' + str(max_fit_steps) + ' ' + working_path+'/fitdata/fitmolecule$i' + ' ' + working_path+'/fitdata/scatter$i.dat' + ' ' + mixture_file + ' ' +'1')\n",
    "                   \n",
    "                   \n",
    "        fout.write('\\n\\ndone')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff033bc5-81a8-4039-bac9-b2eb87e186bb",
   "metadata": {},
   "source": [
    "## Lyzosome Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "35ef0ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists: '/Users/josh/Documents/PhD/TEMP_Carb/CarbonARA/Fitting'\n"
     ]
    }
   ],
   "source": [
    "current = os.getcwd()\n",
    "working = 'Fitting'\n",
    "working_path = os.path.join(current, working)\n",
    "try:\n",
    "    os.mkdir(working_path)\n",
    "    print('Making directory ', working_path)\n",
    "except OSError as error:\n",
    "    print(str(error)[11:])\n",
    "\n",
    "pdb_file = 'Example/Lysozyme/Lysozyme.pdb'\n",
    "\n",
    "# Read in a pdb file\n",
    "M = pdb_2_biobox(pdb_file)\n",
    "\n",
    "# Extract coordinates + sequence\n",
    "coords = extract_CA_coordinates(M)\n",
    "sequence = extract_sequence(M)\n",
    "\n",
    "# Read in DSSP secondary prediction file\n",
    "secondary = read_dssp_file('Example/Lysozyme/Lysozyme_dssp.txt')\n",
    "\n",
    "# Write files ready for Carbonara!\n",
    "write_fingerprint_file(1, sequence, secondary, working_path)\n",
    "write_coordinates_file(coords, working_path)\n",
    "write_mixture_file(working_path)\n",
    "\n",
    "\n",
    "# Define sections to change\n",
    "varying_sections = [5, 9, 11]\n",
    "write_varysections_file(varying_sections, working_path)\n",
    "\n",
    "\n",
    "# Copy SAXS data file into correct format\n",
    "SAXS_file = 'Example/Lysozyme/LysozymeSaxs.dat'\n",
    "copy_saxs(SAXS_file, working_path)\n",
    "\n",
    "try:\n",
    "    os.mkdir(working_path+'/fitdata')\n",
    "    print('Making directory for fit data')\n",
    "except OSError as error:\n",
    "    print(str(error)[11:])\n",
    "    \n",
    "write_sh_file(working_path, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0794407-32c2-4cd4-97f4-50affb4c68e1",
   "metadata": {},
   "source": [
    "### SMARCAL Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "a4869562-8fa8-4445-9df5-61402a9ab8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists: '/Users/josh/Documents/PhD/TEMP_Carb/CarbonARA/Fitting'\n",
      "File exists: '/Users/josh/Documents/PhD/TEMP_Carb/CarbonARA/Fitting/fitdata'\n",
      "/Users/josh/Documents/PhD/TEMP_Carb/CarbonARA/RunMe.sh\n"
     ]
    }
   ],
   "source": [
    "current = os.getcwd()\n",
    "working = 'Fitting'\n",
    "working_path = os.path.join(current, working)\n",
    "try:\n",
    "    os.mkdir(working_path)\n",
    "    print('Making directory ', working_path)\n",
    "except OSError as error:\n",
    "    print(str(error)[11:])\n",
    "\n",
    "pdb_file = 'Example/SMARCAL/human_SMARCAL1.pdb'\n",
    "\n",
    "# Read in a pdb file\n",
    "M = pdb_2_biobox(pdb_file)\n",
    "\n",
    "# Extract coordinates + sequence\n",
    "coords = extract_CA_coordinates(M)\n",
    "sequence = extract_sequence(M)\n",
    "\n",
    "# Read in DSSP secondary prediction file\n",
    "# secondary = read_dssp_file('Example/Lysozyme/Lysozyme_dssp.txt')\n",
    "\n",
    "secondary = ['------SSSSSSS----SSSSSS---HHHHHHHH-----SSS----SSSSSHHHHHHHHHHH-----SSSS---HHHHHH---HHH-------------------HHHHH---HHHHHHHHHHHH---SSSS-------HHHHHHHHHHHHHH---SSSSS----HHHHHHHHHHH-----HHHSSS------------SSSSSHHHH------------SSS---HHHH-----HHHHHHHHHHH---SSSSS--------HHHHHHHHHHH-------HHHHHHHH---SS---SSS------HHHHHHHHHHHH-----HHHH------SSSSSS---HH---HHHHHHHHHHHHHHH-----HHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHH-----SSSS---HHHHHHHHHHHHH----SSSS-------HHHHHHHHHHH-----SSSSS--------------SSSS------HHHHHHHH-----------SSSSSSS-----HHHHHHHHHHHHHHHHH---------HHHH--']\n",
    "\n",
    "# Write files ready for Carbonara!\n",
    "write_fingerprint_file(1, sequence, secondary, working_path)\n",
    "write_coordinates_file(coords, working_path)\n",
    "write_mixture_file(working_path)\n",
    "\n",
    "\n",
    "# Define sections to change\n",
    "varying_sections = [43, 51, 79, 81]\n",
    "write_varysections_file(varying_sections, working_path)\n",
    "\n",
    "\n",
    "# Copy SAXS data file into correct format\n",
    "SAXS_file = 'Example/SMARCAL/smrclcnc_a2.dat'\n",
    "copy_saxs(SAXS_file, working_path)\n",
    "\n",
    "\n",
    "try:\n",
    "    os.mkdir(working_path+'/fitdata')\n",
    "    print('Making directory for fit data')\n",
    "except OSError as error:\n",
    "    print(str(error)[11:])\n",
    "    \n",
    "    \n",
    "write_sh_file(working_path=working_path, fit_n_times=3, min_q=0.02, max_q=0.25, max_fit_steps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b7110a-6e84-4dba-ba90-bf8d1f2b7a83",
   "metadata": {},
   "source": [
    "## Finding secondary structures that should change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "156886c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def section_finder(ss):\n",
    "    \n",
    "    '''Find protein sub-unit sections from the full secondary structure'''\n",
    "    \n",
    "    sections = []\n",
    "    structure_change = np.diff(np.unique(ss, return_inverse=True)[1])\n",
    "\n",
    "    for i, c in enumerate( structure_change ):\n",
    "\n",
    "        if c!=0:\n",
    "            sections.append(ss[i])\n",
    "\n",
    "        if i==structure_change.shape[0]-1:\n",
    "            sections.append(ss[i])\n",
    "            \n",
    "    sections = np.array(sections)\n",
    "    \n",
    "    return sections #, linker_indices #, structure_change\n",
    "\n",
    "\n",
    "def find_linker_indices(sections):\n",
    "    \n",
    "    '''Find linker sub-unit section indices'''\n",
    "    \n",
    "    linker_indices = np.where(sections=='-')[0]\n",
    "    return linker_indices\n",
    "\n",
    "\n",
    "def find_sheet_indices(sections):\n",
    "    \n",
    "    '''Find sheet sub-unit section indices'''\n",
    "\n",
    "    sheet_indices = np.where(sections=='S')[0]\n",
    "    return sheet_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c33a915c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_structures(coords_file, fingerprint_file, linker_indices):\n",
    "    \n",
    "    '''Generate random structures changing one linker section at a time\n",
    "    \n",
    "    Parameters\n",
    "    coords_file:       /path/ to CA coordinates.dat file\n",
    "    fingerprint_file:  /path/ to fingerprint.dat file\n",
    "    linker_indices:    Indices of linker sub-unit sections of protein\n",
    "    \n",
    "    Return\n",
    "    Generated structures are written to ~/rand_structures/.. section_*LINKERINDEX*.dat as xyz\n",
    "    ''' \n",
    "    \n",
    "    current = os.getcwd()\n",
    "    random = 'rand_structures'\n",
    "    random_working = os.path.join(current, random)\n",
    "\n",
    "    try:\n",
    "        os.mkdir(random_working)\n",
    "    except OSError as error:\n",
    "        print(str(error)[11:])\n",
    "\n",
    "    # linker_indices = linker_prep(coords_file, fingerprint_file)\n",
    "    \n",
    "    print('Beginning random structures generation \\n')\n",
    "    for l in tqdm(linker_indices):\n",
    "        \n",
    "        outputname = '/section_'+str(l)\n",
    "        !./generate_structure {fingerprint_file} {coords_file} {random_working}{outputname} {l}\n",
    "        \n",
    "    print('')\n",
    "    print('Finished generating random structures')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f340d9-57e0-417e-88d9-d5194e342d71",
   "metadata": {},
   "source": [
    "### Setup for structure generation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a4de6694",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords_file = 'TestBed/human_SMARCAL2/coordinates1.dat'\n",
    "FP_file = 'TestBed/human_SMARCAL2/fingerPrint1.dat'\n",
    "\n",
    "coords_arr = np.genfromtxt(coords_file)\n",
    "\n",
    "secondary = open(FP_file, 'r').readlines()[-1]\n",
    "secondary = np.asarray(list(secondary))[:-1]\n",
    "\n",
    "sections = section_finder(secondary)\n",
    "linker_indices = find_linker_indices(sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a395766b-77e3-4f96-ab00-e51b71015203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists: '/Users/josh/Documents/PhD/TEMP_Carb/CarbonARA/rand_structures'\n",
      "Beginning random structures generation \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:21<00:00,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished generating random structures\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generate_random_structures(coords_file=coords_file, fingerprint_file=FP_file, linker_indices=linker_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d4f3281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sheet_group_mask(ss):\n",
    "     \n",
    "    '''Groups adjacent sheets in secondary structure file and returns a grouping mask ( 0 : not a sheet;  1+: sheet )\n",
    "    \n",
    "    Parameters\n",
    "    ss (numpy array):            Secondary structure labels (array of strings)\n",
    "    \n",
    "    Returns\n",
    "    sheet_groups (numpy array):  Mask of grouped sheet sections\n",
    "    '''\n",
    "    \n",
    "    sheet_mask = (ss == 'S')*1\n",
    "    sheet_groups = np.zeros(ss.shape[0])\n",
    "    group = 1\n",
    "    \n",
    "    if sheet_mask[0] == 1:\n",
    "        label = True\n",
    "    else:\n",
    "        label = False\n",
    "\n",
    "    for i, c in enumerate(np.diff(sheet_mask)):\n",
    "        \n",
    "        \n",
    "        if c == 1:\n",
    "            label = True\n",
    "\n",
    "        elif c==-1:\n",
    "            label=False\n",
    "            group += 1\n",
    "\n",
    "        else:\n",
    "            pass \n",
    "\n",
    "        if label == True:\n",
    "            if ss[i+1] == 'S':\n",
    "                sheet_groups[i+1] = group\n",
    "                \n",
    "    return sheet_groups\n",
    "\n",
    "\n",
    "def linker_group_mask(ss):\n",
    "    \n",
    "    '''Groups adjacent linkers in secondary structure file and returns a grouping mask ( 0 : not a linker;  1+: linker )\n",
    "    \n",
    "    Parameters\n",
    "    ss (numpy array):             Secondary structure labels (array of strings)\n",
    "    \n",
    "    Returns\n",
    "    linker_groups (numpy array):  Mask of grouped linker sections\n",
    "    '''\n",
    "    \n",
    "    linker_mask = (ss == '-')*1\n",
    "    linker_groups = np.zeros(ss.shape[0])\n",
    "    group = 1\n",
    "    \n",
    "    # checking first index for linker \n",
    "    if linker_mask[0] == 1:\n",
    "        label = True\n",
    "        linker_groups[0] = group\n",
    "    else:\n",
    "        label = False\n",
    "\n",
    "    for i, c in enumerate(np.diff(linker_mask)):\n",
    "    \n",
    "        if c == 1:\n",
    "            label = True\n",
    "\n",
    "        elif c==-1:\n",
    "            label=False\n",
    "            group += 1\n",
    "\n",
    "        else:\n",
    "            pass \n",
    "\n",
    "        if label == True:\n",
    "            \n",
    "            linker_groups[i+1] = group\n",
    "                \n",
    "    return linker_groups #, linker_mask\n",
    "\n",
    "\n",
    "def get_sheet_coords(coords, sheet_groups):\n",
    "\n",
    "    '''Finds CA coordinates of \n",
    "    \n",
    "    Parameters\n",
    "    coords (numpy array):        xyz coordinates of all protein CA atoms\n",
    "    sheet_groups (numpy array):  Mask of grouped sheet sections\n",
    "    \n",
    "    Returns\n",
    "    sheet_coords (numpy array):  xyz coordinates of CA atoms in each sheet structure [ [...sheet 1 coords...] [...sheet 2 coords...] ... ]\n",
    "    '''\n",
    "    \n",
    "    sheet_coords = []\n",
    "\n",
    "    for g in np.unique(sheet_groups):\n",
    "        if g>0:\n",
    "            sheet_coords.append(coords[sheet_groups==g])\n",
    "            \n",
    "    sheet_coords = np.asarray(sheet_coords)\n",
    "    \n",
    "    return sheet_coords\n",
    "\n",
    "\n",
    "def sheet_pairwise_bond_number(sheet_coords, thr=5.5):\n",
    "    \n",
    "    '''Finds the number of pairs of CA atoms within some threshold between all sheet sections\n",
    "    \n",
    "    Parameters\n",
    "    sheet_coords (numpy array): xyz coordinates of CA atoms in each sheet structure [ [...sheet 1 coords...] [...sheet 2 coords...] ... ]\n",
    "    thr (float) {optional}:     Cutoff distance for inter-sheet bonding (default = 5.5 Å)\n",
    "    \n",
    "    Returns\n",
    "    pairwise_bond_num (numpy array): Lower triangular array containing the number of individual CA bonds within threshold between each sheet pair\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    thr = 5.5\n",
    "    number_bonds = 0\n",
    "\n",
    "    pairwise_bond_num = np.zeros([len(sheet_coords), len(sheet_coords)])\n",
    "\n",
    "    for i in range(1,len(sheet_coords)):\n",
    "\n",
    "        for j in range(0,i):\n",
    "\n",
    "            arr1, arr2 = sheet_coords[j], sheet_coords[i]\n",
    "            dist_matrix = cdist(arr1, arr2)\n",
    "            indices = np.where(dist_matrix < thr)\n",
    "\n",
    "            pairwise_bond_num[i,j] = indices[0].shape[0]\n",
    "\n",
    "            number_bonds += indices[0].shape[0]\n",
    "\n",
    "    return pairwise_bond_num  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c7442db2-864d-48a3-b2b2-302126932ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f8/_cxswvtx57j098qpc747tvh80000gn/T/ipykernel_81675/1052980281.py:100: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sheet_coords = np.asarray(sheet_coords)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "dbb21ec3-3465-498a-ab2e-e93082716bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def listdir_nohidden(path):\n",
    "    lst_structs = []\n",
    "    for f in os.listdir(path):\n",
    "        if not f.startswith('.'):\n",
    "            lst_structs.append(f)\n",
    "    return lst_structs\n",
    "\n",
    "\n",
    "def find_breakers(file_dir, linker_indices, sheet_groups):\n",
    "    \n",
    "    \n",
    "    structure_lst = listdir_nohidden(file_dir)\n",
    "\n",
    "    linker_file_dict = {}\n",
    "    for l in linker_indices:\n",
    "        tmp = []\n",
    "\n",
    "        for file in np.sort(structure_lst):\n",
    "            if str(l) == file.split('_')[1]:\n",
    "                tmp.append(file)\n",
    "\n",
    "        linker_file_dict[l] = tmp\n",
    "\n",
    "\n",
    "    linker_bond_dic = {}\n",
    "\n",
    "    for l in linker_indices:\n",
    "        \n",
    "        tmp = []\n",
    "        \n",
    "        for file in linker_file_dict[l]:\n",
    "            coords_file = file_dir+file\n",
    "            coords_arr = np.genfromtxt(coords_file)[:-1]\n",
    "            sheet_coords = get_sheet_coords(coords_arr, sheet_groups)\n",
    "            tmp.append(sheet_pairwise_bond_number(sheet_coords))\n",
    "    \n",
    "        linker_bond_dic[l] = tmp\n",
    "        \n",
    "        \n",
    "    return linker_bond_dic\n",
    "        \n",
    "    \n",
    "def get_section_groups(ss):\n",
    "    \n",
    "    structure_change = np.diff(np.unique(ss, return_inverse=True)[1])\n",
    "    group = 0\n",
    "    structural_groups = np.zeros(ss.shape)\n",
    "    structural_groups[0] = group\n",
    "\n",
    "    for i, c in enumerate(structure_change):\n",
    "\n",
    "        if c != 0:\n",
    "            group += 1\n",
    "\n",
    "        structural_groups[i+1] = group\n",
    "    return structural_groups\n",
    "\n",
    "\n",
    "def get_varying_sections(coords_file, FP_file):\n",
    "    \n",
    "    coords_arr = np.genfromtxt(coords_file)\n",
    "\n",
    "    secondary = open(FP_file, 'r').readlines()[-1]\n",
    "    secondary = np.asarray(list(secondary))[:-1]\n",
    "    \n",
    "    sections = section_finder(secondary)\n",
    "    linker_indices = find_linker_indices(sections)\n",
    "    \n",
    "    # Make new structures\n",
    "    generate_random_structures(coords_file=coords_file, fingerprint_file=FP_file, linker_indices=linker_indices)\n",
    "    \n",
    "    sheet_groups = sheet_group_mask(secondary)\n",
    "    sheet_coords = get_sheet_coords(coords_arr, sheet_groups)\n",
    "    \n",
    "    linker_bond_dic = find_breakers('rand_structures/', linker_indices, sheet_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d66246e-9fea-434c-80d7-82cfcae7030b",
   "metadata": {},
   "source": [
    "### Find pairwise bonds for all generated structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5ba2361f-d990-4c5d-9a82-d419204b755a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f8/_cxswvtx57j098qpc747tvh80000gn/T/ipykernel_81675/1052980281.py:100: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sheet_coords = np.asarray(sheet_coords)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e8e06d0-6229-43bd-883e-65f8ff7ce125",
   "metadata": {},
   "source": [
    "### Reference structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0eb3420b-5991-4d54-8d2a-5e9a0c4e36ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f8/_cxswvtx57j098qpc747tvh80000gn/T/ipykernel_81675/1052980281.py:100: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sheet_coords = np.asarray(sheet_coords)\n"
     ]
    }
   ],
   "source": [
    "coords_arr_ref = np.genfromtxt('TestBed/human_SMARCAL2/coordinates1.dat')\n",
    "sheet_coords_ref = get_sheet_coords(coords_arr_ref, sheet_groups)\n",
    "ref_bonds = sheet_pairwise_bond_number(sheet_coords_ref)\n",
    "\n",
    "bonding_breaks = {}\n",
    "average_bond_breaks = []\n",
    "\n",
    "for d in linker_bond_dic:\n",
    "    \n",
    "    tmp = []\n",
    "    for i, new_bonds in enumerate(linker_bond_dic[d]):\n",
    "        if i == 0:\n",
    "            ref = new_bonds\n",
    "            \n",
    "        \n",
    "        tmp.append( (ref != new_bonds).sum() )\n",
    "        \n",
    "    bonding_breaks[d] = tmp\n",
    "    average_bond_breaks.append(np.mean(tmp))\n",
    "    \n",
    "average_bond_breaks = np.array(average_bond_breaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "c3d33f56-d771-4ce5-8765-b73579406153",
   "metadata": {},
   "outputs": [],
   "source": [
    "section_groups = get_section_groups(secondary)\n",
    "\n",
    "linker_len_thr = 4\n",
    "\n",
    "varying_sections = []\n",
    "\n",
    "for l, bb in zip(linker_indices, average_bond_breaks):\n",
    "\n",
    "    if bb == 0:\n",
    "        if (section_groups==l).sum() >= linker_len_thr:\n",
    "            varying_sections.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "b979882f-1d88-4f88-83a7-da1712a88beb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[43, 51, 79, 81]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "varying_sections"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
