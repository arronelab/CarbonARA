{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd6ee0a0-39ab-4ffb-a347-98f4d76a8ec8",
   "metadata": {},
   "source": [
    "#### Uncomment to install biobox - used in PDB parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aec8c668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install biobox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe6fb0c-e649-42c3-8899-e75b207811c3",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Imports + functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "03241443",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import biobox as bb\n",
    "from tqdm import tqdm\n",
    "\n",
    "def setup_working_directory():\n",
    "    \n",
    "    current = os.getcwd()\n",
    "    working = 'Fitting'\n",
    "    working_path = os.path.join(current, working)\n",
    "    try:\n",
    "        os.mkdir(working_path)\n",
    "        print('Making directory ', working_path)\n",
    "    except OSError as error:\n",
    "        print(str(error)[11:])\n",
    "        \n",
    "    try:\n",
    "        os.mkdir(working_path+'/fitdata')\n",
    "        print('Making directory for fit data')\n",
    "    except OSError as error:\n",
    "        print(str(error)[11:])\n",
    "    print('Complete')\n",
    "    return working_path\n",
    "\n",
    "\n",
    "def pdb_2_biobox(pdb_file):\n",
    "    M = bb.Molecule()\n",
    "    M.import_pdb(pdb_file)\n",
    "    return M\n",
    "\n",
    "\n",
    "def extract_CA_coordinates(M):\n",
    "    ca_idx = (M.data['name']=='CA').values\n",
    "    ca_coords = M.coordinates[0][ca_idx]\n",
    "    \n",
    "    if ca_coords.shape[0] != M.data['resid'].nunique():\n",
    "        raise Exception(\"You better check your PDB... The number of CA atoms does not equal the number of ResIDs in your PDB file!\") \n",
    "    else:\n",
    "        return ca_coords\n",
    "\n",
    "    \n",
    "def extract_sequence(M):\n",
    "    \n",
    "    \n",
    "    aa_names = {\n",
    "                'A': 'ALA', 'C': 'CYS', 'D': 'ASP', 'E': 'GLU',\n",
    "                'F': 'PHE', 'G': 'GLY', 'H': 'HIS', 'I': 'ILE',\n",
    "                'K': 'LYS', 'L': 'LEU', 'M': 'MET', 'N': 'ASN',\n",
    "                'P': 'PRO', 'Q': 'GLN', 'R': 'ARG', 'S': 'SER',\n",
    "                'T': 'THR', 'V': 'VAL', 'W': 'TRP', 'Y': 'TYR'\n",
    "                }\n",
    "\n",
    "    names_aa = {y: x for x, y in aa_names.items()}\n",
    "    \n",
    "    ca_idx = (M.data['name']=='CA').values\n",
    "    resnames = M.data['resname'][ca_idx].map(names_aa).values\n",
    "    \n",
    "    if resnames.shape[0] != M.data['resid'].nunique():\n",
    "        raise Exception(\"You better check your PDB... The number of CA atoms does not equal the number of ResIDs in your PDB file!\") \n",
    "    else:\n",
    "        return resnames\n",
    "\n",
    "\n",
    "def write_fingerprint_file(number_chains, sequence, secondary_structure, working_path):\n",
    "    \n",
    "    assert isinstance(number_chains, int), 'Yikes... The number of chains is not int type!'\n",
    "    \n",
    "    if number_chains > 1:\n",
    "        print('Are sure you have more than one chain - if not this will cause segmentation errors later! You have been warned...')\n",
    "    \n",
    "    seq_run = ''.join(list(sequence))\n",
    "    ss_run = ''.join(list(secondary_structure))\n",
    "    \n",
    "    if len(seq_run) != len(ss_run):\n",
    "        raise Exception(\"Uh Oh... The length of sequence and secondary structure is not equal!\") \n",
    "    \n",
    "    f = open(working_path+\"/fingerPrint1.dat\", \"w\")\n",
    "    f.write(str(number_chains))\n",
    "    f.write('\\n \\n')\n",
    "    f.write(seq_run)\n",
    "    f.write('\\n \\n')\n",
    "    f.write(ss_run)\n",
    "    f.close()\n",
    "    \n",
    "    \n",
    "def write_coordinates_file(ca_coords, working_path):\n",
    "    \n",
    "    assert type(coords).__module__ == np.__name__, 'Thats never good... the CA coordinates are not a numpy array'\n",
    "    np.savetxt(working_path+'/coordinates1.dat', coords, delimiter=' ', fmt='%s',newline='\\n', header='', footer='')\n",
    "    \n",
    "    \n",
    "def write_mixture_file(working_path):\n",
    "    # if default:\n",
    "    f = open(working_path+\"/mixtureFile.dat\", \"w\")\n",
    "    f.write(str(1))\n",
    "        \n",
    "#     else:\n",
    "#          copy input file\n",
    "\n",
    "\n",
    "def write_varysections_file(varying_sections, working_path):\n",
    "    # auto: run beta sheet breaking code; write output sections to file\n",
    "    f = open(working_path+\"/varyingSectionSecondary1.dat\", \"w\")\n",
    "    for i, s in enumerate(varying_sections):\n",
    "        f.write(str(s))\n",
    "        \n",
    "        if i < len(varying_sections)-1:\n",
    "            f.write('\\n')\n",
    "    f.close()\n",
    "\n",
    "    \n",
    "def copy_saxs(SAXS_file, working_path):\n",
    "    \n",
    "    saxs_arr = np.genfromtxt(SAXS_file)\n",
    "    \n",
    "    if saxs_arr.shape[1] == 3:\n",
    "        saxs_arr = saxs_arr[:,:2]\n",
    "        \n",
    "    np.savetxt(working_path+'/Saxs.dat', saxs_arr, delimiter=' ', fmt='%s',newline='\\n', header='', footer='')\n",
    "\n",
    "\n",
    "def read_dssp_file(dssp_filename):\n",
    "    \n",
    "    simplify_dict = {'H': 'H', 'B': 'S', 'E': 'S', 'G': 'H', 'I': 'H', 'T': '-', 'S': '-', '-': '-', ' ': '-'}\n",
    "    \n",
    "    lines=[]\n",
    "    with open(dssp_filename) as input_data:\n",
    "        # Skips text before the beginning of the interesting block:\n",
    "        for line in input_data:\n",
    "            if line.strip() == '#  RESIDUE AA STRUCTURE BP1 BP2  ACC     N-H-->O    O-->H-N    N-H-->O    O-->H-N    TCO  KAPPA ALPHA  PHI   PSI    X-CA   Y-CA   Z-CA': \n",
    "                break\n",
    "        # Reads text until the end of the block:\n",
    "        for line in input_data:  # This keeps reading the file\n",
    "            lines.append(simplify_dict[line[16]])\n",
    "    return ''.join(lines)\n",
    "    \n",
    "    \n",
    "def simplify_secondary(dssp_struct):\n",
    "    \n",
    "    simplify_dict = {'H': 'H', 'B': 'S', 'E': 'S', 'G': 'H', 'I': 'H', 'T': '-', 'S': '-', '-': '-', ' ': '-'}\n",
    "    \n",
    "    secondary_structure = []\n",
    "    \n",
    "    for s in dssp_struct:\n",
    "        \n",
    "        if s not in list(simplify_dict.keys()):\n",
    "            print('>>> ', s, ' <<<')\n",
    "            raise Exception('Secondary structure not recognised!')\n",
    "            \n",
    "        secondary_structure.append(simplify_dict[s])\n",
    "        \n",
    "    return secondary_structure\n",
    "\n",
    "\n",
    "def write_sh_file(working_path, fit_n_times, min_q, max_q, max_fit_steps):\n",
    "    \n",
    "    curr = os.getcwd()\n",
    "    run_file = curr + '/RunMe.sh'\n",
    "\n",
    "    with open(run_file, 'w+') as fout:\n",
    "        fout.write('#!/bin/bash')\n",
    "        \n",
    "        saxs_file = working_path+'/Saxs.dat'\n",
    "        FP_file = working_path+\"/fingerPrint1.dat\"\n",
    "        coords_file = working_path+'/coordinates1.dat'\n",
    "        varying_file = working_path+\"/varyingSectionSecondary1.dat\"\n",
    "        mixture_file = working_path+\"/mixtureFile.dat\"\n",
    "        \n",
    "        # Auto assign min / max q from SAXS profile\n",
    "        # saxs_arr = np.genfromtxt(saxs_file)\n",
    "        # min_q = np.round(saxs_arr[:,0].min(),2)\n",
    "        # max_q = np.round(saxs_arr[:,0].max(),2)\n",
    "        \n",
    "        fout.write('\\nfor i in {1..'+str(fit_n_times)+'}')\n",
    "\n",
    "        fout.write('\\n\\ndo')\n",
    "        fout.write('\\n\\n   echo \" Run number : $i \"')\n",
    "        fout.write('\\n\\n   ./predictStructure ' + saxs_file + ' ' + working_path+'/' + ' ' + coords_file + ' ' + 'none' + ' ' + varying_file + ' ' + '1' + ' ' + 'none' + \\\n",
    "                   ' ' + 'none' + ' ' + str(min_q) + ' ' + str(max_q) + ' ' + str(max_fit_steps) + ' ' + working_path+'/fitdata/fitmolecule$i' + ' ' + working_path+'/fitdata/scatter$i.dat' + ' ' + mixture_file + ' ' +'1')\n",
    "                   \n",
    "        fout.write('\\n\\ndone')\n",
    "        \n",
    "    print('Successfully written bash script to: ', working_path+run_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff033bc5-81a8-4039-bac9-b2eb87e186bb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Lysozyme Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "77c00416-8cfb-4270-a5d0-86781bb5762b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists: '/Users/josh/Documents/PhD/TEMP_Carb/CarbonARA/Fitting'\n"
     ]
    }
   ],
   "source": [
    "working_path = setup_working_directory()\n",
    "pdb_file = 'Example/Lysozyme/Lysozyme.pdb'\n",
    "\n",
    "# Read in a pdb file\n",
    "M = pdb_2_biobox(pdb_file)\n",
    "\n",
    "# Extract coordinates + sequence\n",
    "coords = extract_CA_coordinates(M)\n",
    "sequence = extract_sequence(M)\n",
    "\n",
    "# Read in DSSP secondary prediction file\n",
    "secondary = read_dssp_file('Example/Lysozyme/Lysozyme_dssp.txt')\n",
    "\n",
    "# Write files ready for Carbonara!\n",
    "write_fingerprint_file(1, sequence, secondary, working_path)\n",
    "write_coordinates_file(coords, working_path)\n",
    "write_mixture_file(working_path)\n",
    "\n",
    "\n",
    "# Define sections to change\n",
    "varying_sections = [5, 9, 11]\n",
    "write_varysections_file(varying_sections, working_path)\n",
    "\n",
    "\n",
    "# Copy SAXS data file into correct format\n",
    "SAXS_file = 'Example/Lysozyme/LysozymeSaxs.dat'\n",
    "copy_saxs(SAXS_file, working_path)\n",
    "\n",
    "try:\n",
    "    os.mkdir(working_path+'/fitdata')\n",
    "    print('Making directory for fit data')\n",
    "except OSError as error:\n",
    "    print(str(error)[11:])\n",
    "    \n",
    "write_sh_file(working_path, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0794407-32c2-4cd4-97f4-50affb4c68e1",
   "metadata": {},
   "source": [
    "# SMARCAL Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39b3ba6-1d4e-42e1-b6be-a3a77bab6f93",
   "metadata": {},
   "source": [
    "#### Create a working directory for Carbonara - input files & predictions will be written to this directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "62674796-412c-43d8-b4d7-4969537a4f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making directory  /Users/josh/Documents/PhD/TEMP_Carb/CarbonARA/Fitting\n",
      "Making directory for fit data\n"
     ]
    }
   ],
   "source": [
    "working_path = setup_working_directory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ad8b5f-9753-4615-a930-d89681009018",
   "metadata": {},
   "source": [
    "#### Provide a PDB containing the known structure for protein - xyz coordinates and sequence extracted from PDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "e4414bfb-6796-46ef-86c2-cccd87028131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a PDB file location\n",
    "pdb_file = 'Example/SMARCAL/human_SMARCAL1.pdb'\n",
    "\n",
    "# Read in a pdb file\n",
    "M = pdb_2_biobox(pdb_file)\n",
    "\n",
    "# Extract coordinates + primary sequence\n",
    "coords = extract_CA_coordinates(M)\n",
    "sequence = extract_sequence(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606c6a00-e90c-4812-89bd-78f60f97775c",
   "metadata": {},
   "source": [
    "#### Carbonara requires the protein's secondary structure - user can copy and paste a string or give a DSSP file!\n",
    "\n",
    "> Note: Carbonara uses a simplified secondary structure dictionary: H (alpha helices), S (beta strands), and - (Linkers/Loops) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "7de31d55-0e52-4406-84d0-9791639a203b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually defining the simplified secondary structure string\n",
    "secondary = ['------SSSSSSS----SSSSSS---HHHHHHHH-----SSS----SSSSSHHHHHHHHHHH-----SSSS---HHHHHH---HHH-------------------HHHHH---HHHHHHHHHHHH---SSSS-------HHHHHHHHHHHHHH---SSSSS----HHHHHHHHHHH-----HHHSSS------------SSSSSHHHH------------SSS---HHHH-----HHHHHHHHHHH---SSSSS--------HHHHHHHHHHH-------HHHHHHHH---SS---SSS------HHHHHHHHHHHH-----HHHH------SSSSSS---HH---HHHHHHHHHHHHHHH-----HHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHH-----SSSS---HHHHHHHHHHHHH----SSSS-------HHHHHHHHHHH-----SSSSS--------------SSSS------HHHHHHHH-----------SSSSSSS-----HHHHHHHHHHHHHHHHH---------HHHH--']\n",
    "\n",
    "# --- Simplify your secondary structure string ---\n",
    "\n",
    "# Have DSSP secondary structure string? Simplify with:\n",
    "# secondary = simplify_secondary(DSSP_string)\n",
    "\n",
    "# --- DSSP file procedure ---\n",
    "\n",
    "# Just have a DSSP file?\n",
    "# secondary = read_dssp_file(DSSP_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a8943a-790f-442a-ab56-c5fb73d13739",
   "metadata": {},
   "source": [
    "#### Writing files to be used as input for Carbonara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "c05512b8-ad6a-4f31-9394-c31a49e157af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write files ready for Carbonara!\n",
    "write_fingerprint_file(1, sequence, secondary, working_path)\n",
    "write_coordinates_file(coords, working_path)\n",
    "write_mixture_file(working_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e065974a-9799-4992-a51b-76b5659e66a2",
   "metadata": {},
   "source": [
    "#### Sections to be resampled - User can manually select these or have these automatically assigned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "51dcfe46-eea9-4a57-9366-4610aaf60172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually defining sections to change\n",
    "varying_sections = [43, 51, 79, 81]  # <<< Give us your selection! #\n",
    "write_varysections_file(varying_sections, working_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384a7eae-e11b-4339-a436-d29ac1cc5a68",
   "metadata": {},
   "source": [
    "#### Provide a SAXS profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "a8875887-161f-47f0-8009-2a0d92133cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy SAXS data file into correct format\n",
    "SAXS_file = 'Example/SMARCAL/smrclcnc_a2.dat'\n",
    "copy_saxs(SAXS_file, working_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bf4daf-6910-4314-b458-41e2be61e6a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Generate bash file to run Carbonara\n",
    "\n",
    "**Inputs**\n",
    " > fit_n_times   : number of unique fits to be generated (sequentially)\n",
    " \n",
    " > min_q         : lower bound q in SAXS data\n",
    " \n",
    " > max_q         : upper bound q in SAXS data\n",
    " \n",
    " > max_fit_steps : maximum number of fitting iterations to be performed (recommend 4000-10,000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "956339f0-6dcd-4097-a69e-3c35e559faac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully written bash script to:  /Users/josh/Documents/PhD/TEMP_Carb/CarbonARA/Fitting/Users/josh/Documents/PhD/TEMP_Carb/CarbonARA/RunMe.sh\n"
     ]
    }
   ],
   "source": [
    "write_sh_file(working_path=working_path, fit_n_times=3, min_q=0.02, max_q=0.25, max_fit_steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "11b0bb01-2498-41e8-9f8d-76aa5166e763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Run number : 1 \n",
      " Run number : 2 \n",
      " Run number : 3 \n"
     ]
    }
   ],
   "source": [
    "!sh RunMe.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075141ee-788d-463c-961c-accf3c17ce1f",
   "metadata": {},
   "source": [
    "**********"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b7110a-6e84-4dba-ba90-bf8d1f2b7a83",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Automatically finding varying sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "156886c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def section_finder(ss):\n",
    "    \n",
    "    '''Find protein sub-unit sections from the full secondary structure'''\n",
    "    \n",
    "    sections = []\n",
    "    structure_change = np.diff(np.unique(ss, return_inverse=True)[1])\n",
    "\n",
    "    for i, c in enumerate( structure_change ):\n",
    "\n",
    "        if c!=0:\n",
    "            sections.append(ss[i])\n",
    "\n",
    "        if i==structure_change.shape[0]-1:\n",
    "            sections.append(ss[i])\n",
    "            \n",
    "    sections = np.array(sections)\n",
    "    \n",
    "    return sections #, linker_indices #, structure_change\n",
    "\n",
    "\n",
    "def find_linker_indices(sections):\n",
    "    \n",
    "    '''Find linker sub-unit section indices'''\n",
    "    \n",
    "    linker_indices = np.where(sections=='-')[0]\n",
    "    return linker_indices\n",
    "\n",
    "\n",
    "def find_sheet_indices(sections):\n",
    "    \n",
    "    '''Find sheet sub-unit section indices'''\n",
    "\n",
    "    sheet_indices = np.where(sections=='S')[0]\n",
    "    return sheet_indices\n",
    "\n",
    "\n",
    "def generate_random_structures(coords_file, fingerprint_file, linker_indices):\n",
    "    \n",
    "    '''Generate random structures changing one linker section at a time\n",
    "    \n",
    "    Parameters\n",
    "    coords_file:       /path/ to CA coordinates.dat file\n",
    "    fingerprint_file:  /path/ to fingerprint.dat file\n",
    "    linker_indices:    Indices of linker sub-unit sections of protein\n",
    "    \n",
    "    Return\n",
    "    Generated structures are written to ~/rand_structures/.. section_*LINKERINDEX*.dat as xyz\n",
    "    ''' \n",
    "    \n",
    "    current = os.getcwd()\n",
    "    random = 'rand_structures'\n",
    "    random_working = os.path.join(current, random)\n",
    "\n",
    "    try:\n",
    "        os.mkdir(random_working)\n",
    "    except OSError as error:\n",
    "        print(str(error)[11:])\n",
    "\n",
    "    # linker_indices = linker_prep(coords_file, fingerprint_file)\n",
    "    \n",
    "    print('Beginning random structures generation \\n')\n",
    "    for l in tqdm(linker_indices):\n",
    "        \n",
    "        outputname = '/section_'+str(l)\n",
    "        !./generate_structure {fingerprint_file} {coords_file} {random_working}{outputname} {l}\n",
    "        \n",
    "    print('')\n",
    "    print('Finished generating random structures')\n",
    "    \n",
    "    \n",
    "def sheet_group_mask(ss):\n",
    "     \n",
    "    '''Groups adjacent sheets in secondary structure file and returns a grouping mask ( 0 : not a sheet;  1+: sheet )\n",
    "    \n",
    "    Parameters\n",
    "    ss (numpy array):            Secondary structure labels (array of strings)\n",
    "    \n",
    "    Returns\n",
    "    sheet_groups (numpy array):  Mask of grouped sheet sections\n",
    "    '''\n",
    "    \n",
    "    sheet_mask = (ss == 'S')*1\n",
    "    sheet_groups = np.zeros(ss.shape[0])\n",
    "    group = 1\n",
    "    \n",
    "    if sheet_mask[0] == 1:\n",
    "        label = True\n",
    "    else:\n",
    "        label = False\n",
    "\n",
    "    for i, c in enumerate(np.diff(sheet_mask)):\n",
    "        \n",
    "        \n",
    "        if c == 1:\n",
    "            label = True\n",
    "\n",
    "        elif c==-1:\n",
    "            label=False\n",
    "            group += 1\n",
    "\n",
    "        else:\n",
    "            pass \n",
    "\n",
    "        if label == True:\n",
    "            if ss[i+1] == 'S':\n",
    "                sheet_groups[i+1] = group\n",
    "                \n",
    "    return sheet_groups\n",
    "\n",
    "\n",
    "def linker_group_mask(ss):\n",
    "    \n",
    "    '''Groups adjacent linkers in secondary structure file and returns a grouping mask ( 0 : not a linker;  1+: linker )\n",
    "    \n",
    "    Parameters\n",
    "    ss (numpy array):             Secondary structure labels (array of strings)\n",
    "    \n",
    "    Returns\n",
    "    linker_groups (numpy array):  Mask of grouped linker sections\n",
    "    '''\n",
    "    \n",
    "    linker_mask = (ss == '-')*1\n",
    "    linker_groups = np.zeros(ss.shape[0])\n",
    "    group = 1\n",
    "    \n",
    "    # checking first index for linker \n",
    "    if linker_mask[0] == 1:\n",
    "        label = True\n",
    "        linker_groups[0] = group\n",
    "    else:\n",
    "        label = False\n",
    "\n",
    "    for i, c in enumerate(np.diff(linker_mask)):\n",
    "    \n",
    "        if c == 1:\n",
    "            label = True\n",
    "\n",
    "        elif c==-1:\n",
    "            label=False\n",
    "            group += 1\n",
    "\n",
    "        else:\n",
    "            pass \n",
    "\n",
    "        if label == True:\n",
    "            \n",
    "            linker_groups[i+1] = group\n",
    "                \n",
    "    return linker_groups #, linker_mask\n",
    "\n",
    "\n",
    "def get_sheet_coords(coords, sheet_groups):\n",
    "\n",
    "    '''Finds CA coordinates of \n",
    "    \n",
    "    Parameters\n",
    "    coords (numpy array):        xyz coordinates of all protein CA atoms\n",
    "    sheet_groups (numpy array):  Mask of grouped sheet sections\n",
    "    \n",
    "    Returns\n",
    "    sheet_coords (numpy array):  xyz coordinates of CA atoms in each sheet structure [ [...sheet 1 coords...] [...sheet 2 coords...] ... ]\n",
    "    '''\n",
    "    \n",
    "    sheet_coords = []\n",
    "\n",
    "    for g in np.unique(sheet_groups):\n",
    "        if g>0:\n",
    "            sheet_coords.append(coords[sheet_groups==g])\n",
    "            \n",
    "    sheet_coords = np.asarray(sheet_coords)\n",
    "    \n",
    "    return sheet_coords\n",
    "\n",
    "\n",
    "def sheet_pairwise_bond_number(sheet_coords, thr=5.5):\n",
    "    \n",
    "    '''Finds the number of pairs of CA atoms within some threshold between all sheet sections\n",
    "    \n",
    "    Parameters\n",
    "    sheet_coords (numpy array): xyz coordinates of CA atoms in each sheet structure [ [...sheet 1 coords...] [...sheet 2 coords...] ... ]\n",
    "    thr (float) {optional}:     Cutoff distance for inter-sheet bonding (default = 5.5 Å)\n",
    "    \n",
    "    Returns\n",
    "    pairwise_bond_num (numpy array): Lower triangular array containing the number of individual CA bonds within threshold between each sheet pair\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    thr = 5.5\n",
    "    number_bonds = 0\n",
    "\n",
    "    pairwise_bond_num = np.zeros([len(sheet_coords), len(sheet_coords)])\n",
    "\n",
    "    for i in range(1,len(sheet_coords)):\n",
    "\n",
    "        for j in range(0,i):\n",
    "\n",
    "            arr1, arr2 = sheet_coords[j], sheet_coords[i]\n",
    "            dist_matrix = cdist(arr1, arr2)\n",
    "            indices = np.where(dist_matrix < thr)\n",
    "\n",
    "            pairwise_bond_num[i,j] = indices[0].shape[0]\n",
    "\n",
    "            number_bonds += indices[0].shape[0]\n",
    "\n",
    "    return pairwise_bond_num  \n",
    "\n",
    "\n",
    "def listdir_nohidden(path):\n",
    "    lst_structs = []\n",
    "    for f in os.listdir(path):\n",
    "        if not f.startswith('.'):\n",
    "            lst_structs.append(f)\n",
    "    return lst_structs\n",
    "\n",
    "\n",
    "def find_breakers(file_dir, linker_indices, sheet_groups):\n",
    "    \n",
    "    \n",
    "    structure_lst = listdir_nohidden(file_dir)\n",
    "\n",
    "    linker_file_dict = {}\n",
    "    for l in linker_indices:\n",
    "        tmp = []\n",
    "\n",
    "        for file in np.sort(structure_lst):\n",
    "            if str(l) == file.split('_')[1]:\n",
    "                tmp.append(file)\n",
    "\n",
    "        linker_file_dict[l] = tmp\n",
    "\n",
    "\n",
    "    linker_bond_dic = {}\n",
    "\n",
    "    for l in linker_indices:\n",
    "        \n",
    "        tmp = []\n",
    "        \n",
    "        for file in linker_file_dict[l]:\n",
    "            coords_file = file_dir+file\n",
    "            coords_arr = np.genfromtxt(coords_file)[:-1]\n",
    "            sheet_coords = get_sheet_coords(coords_arr, sheet_groups)\n",
    "            tmp.append(sheet_pairwise_bond_number(sheet_coords))\n",
    "    \n",
    "        linker_bond_dic[l] = tmp\n",
    "        \n",
    "        \n",
    "    return linker_bond_dic\n",
    "        \n",
    "    \n",
    "def get_section_groups(ss):\n",
    "    \n",
    "    structure_change = np.diff(np.unique(ss, return_inverse=True)[1])\n",
    "    group = 0\n",
    "    structural_groups = np.zeros(ss.shape)\n",
    "    structural_groups[0] = group\n",
    "\n",
    "    for i, c in enumerate(structure_change):\n",
    "\n",
    "        if c != 0:\n",
    "            group += 1\n",
    "\n",
    "        structural_groups[i+1] = group\n",
    "    return structural_groups\n",
    "\n",
    "\n",
    "def get_varying_sections(coords_file, FP_file, gen=False):\n",
    "    \n",
    "    coords_arr = np.genfromtxt(coords_file)\n",
    "\n",
    "    secondary = open(FP_file, 'r').readlines()[-1]\n",
    "    secondary = np.asarray(list(secondary))[:-1]\n",
    "    \n",
    "    sections = section_finder(secondary)\n",
    "    linker_indices = find_linker_indices(sections)\n",
    "    \n",
    "    if gen:\n",
    "        # Make new structures\n",
    "        generate_random_structures(coords_file=coords_file, fingerprint_file=FP_file, linker_indices=linker_indices)\n",
    "    \n",
    "    \n",
    "    sheet_groups = sheet_group_mask(secondary)\n",
    "    print(coords_arr.shape, sheet_groups.shape)\n",
    "    \n",
    "    print(coords_arr)\n",
    "\n",
    "\n",
    "    sheet_coords = get_sheet_coords(coords_arr, sheet_groups)\n",
    "\n",
    "    \n",
    "    linker_bond_dic = find_breakers('rand_structures/', linker_indices, sheet_groups)\n",
    "    \n",
    "    coords_arr_ref = coords_arr.copy()\n",
    "    sheet_coords_ref = get_sheet_coords(coords_arr_ref, sheet_groups)\n",
    "    ref_bonds = sheet_pairwise_bond_number(sheet_coords_ref)\n",
    "    \n",
    "\n",
    "    bonding_breaks = {}\n",
    "    average_bond_breaks = []\n",
    "\n",
    "    for d in linker_bond_dic:\n",
    "\n",
    "        tmp = []\n",
    "        for i, new_bonds in enumerate(linker_bond_dic[d]):\n",
    "            if i == 0:\n",
    "                ref = new_bonds\n",
    "\n",
    "\n",
    "            tmp.append( (ref != new_bonds).sum() )\n",
    "\n",
    "        bonding_breaks[d] = tmp\n",
    "        average_bond_breaks.append(np.mean(tmp))\n",
    "\n",
    "    average_bond_breaks = np.array(average_bond_breaks)\n",
    "    \n",
    "    \n",
    "    section_groups = get_section_groups(secondary)\n",
    "\n",
    "    linker_len_thr = 4\n",
    "\n",
    "    varying_sections = []\n",
    "\n",
    "    for l, bb in zip(linker_indices, average_bond_breaks):\n",
    "\n",
    "        if bb == 0:\n",
    "            if (section_groups==l).sum() >= linker_len_thr:\n",
    "                varying_sections.append(l)\n",
    "    \n",
    "    return varying_sections\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c33a915c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f340d9-57e0-417e-88d9-d5194e342d71",
   "metadata": {},
   "source": [
    "### Setup for structure generation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a4de6694",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords_file = 'TestBed/human_SMARCAL2/coordinates1.dat'\n",
    "FP_file = 'TestBed/human_SMARCAL2/fingerPrint1.dat'\n",
    "\n",
    "coords_arr = np.genfromtxt(coords_file)\n",
    "\n",
    "secondary = open(FP_file, 'r').readlines()[-1]\n",
    "secondary = np.asarray(list(secondary))[:-1]\n",
    "\n",
    "sections = section_finder(secondary)\n",
    "linker_indices = find_linker_indices(sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a395766b-77e3-4f96-ab00-e51b71015203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists: '/Users/josh/Documents/PhD/TEMP_Carb/CarbonARA/rand_structures'\n",
      "Beginning random structures generation \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:21<00:00,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished generating random structures\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generate_random_structures(coords_file=coords_file, fingerprint_file=FP_file, linker_indices=linker_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "f4c9d848-e1f4-4d38-9211-4bb679141280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(546, 3) (545,)\n",
      "[[-13.201  -6.296 -53.957]\n",
      " [-10.703  -3.406 -53.291]\n",
      " [ -9.54   -3.087 -49.639]\n",
      " ...\n",
      " [ 16.099  17.297  -8.711]\n",
      " [ 17.364  18.256  -5.181]\n",
      " [ 19.849  15.283  -4.758]]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "boolean index did not match indexed array along dimension 0; dimension is 546 but corresponding boolean dimension is 545",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [308]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m coords_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTestBed/human_SMARCAL2/coordinates1.dat\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m FP_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTestBed/human_SMARCAL2/fingerPrint1.dat\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mget_varying_sections\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoords_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mFP_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [307]\u001b[0m, in \u001b[0;36mget_varying_sections\u001b[0;34m(coords_file, FP_file, gen)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(coords_arr\u001b[38;5;241m.\u001b[39mshape, sheet_groups\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mprint\u001b[39m(coords_arr)\n\u001b[0;32m---> 80\u001b[0m sheet_coords \u001b[38;5;241m=\u001b[39m \u001b[43mget_sheet_coords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoords_arr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msheet_groups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m linker_bond_dic \u001b[38;5;241m=\u001b[39m find_breakers(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrand_structures/\u001b[39m\u001b[38;5;124m'\u001b[39m, linker_indices, sheet_groups)\n\u001b[1;32m     85\u001b[0m coords_arr_ref \u001b[38;5;241m=\u001b[39m coords_arr\u001b[38;5;241m.\u001b[39mcopy()\n",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36mget_sheet_coords\u001b[0;34m(coords, sheet_groups)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39munique(sheet_groups):\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m g\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 98\u001b[0m         sheet_coords\u001b[38;5;241m.\u001b[39mappend(\u001b[43mcoords\u001b[49m\u001b[43m[\u001b[49m\u001b[43msheet_groups\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43mg\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m    100\u001b[0m sheet_coords \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(sheet_coords)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sheet_coords\n",
      "\u001b[0;31mIndexError\u001b[0m: boolean index did not match indexed array along dimension 0; dimension is 546 but corresponding boolean dimension is 545"
     ]
    }
   ],
   "source": [
    "coords_file = 'TestBed/human_SMARCAL2/coordinates1.dat'\n",
    "FP_file = 'TestBed/human_SMARCAL2/fingerPrint1.dat'\n",
    "\n",
    "get_varying_sections(coords_file, FP_file, gen=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d66246e-9fea-434c-80d7-82cfcae7030b",
   "metadata": {},
   "source": [
    "### Find pairwise bonds for all generated structures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8e06d0-6229-43bd-883e-65f8ff7ce125",
   "metadata": {},
   "source": [
    "### Reference structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0eb3420b-5991-4d54-8d2a-5e9a0c4e36ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f8/_cxswvtx57j098qpc747tvh80000gn/T/ipykernel_81675/1052980281.py:100: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sheet_coords = np.asarray(sheet_coords)\n"
     ]
    }
   ],
   "source": [
    "coords_arr_ref = np.genfromtxt('TestBed/human_SMARCAL2/coordinates1.dat')\n",
    "sheet_coords_ref = get_sheet_coords(coords_arr_ref, sheet_groups)\n",
    "ref_bonds = sheet_pairwise_bond_number(sheet_coords_ref)\n",
    "\n",
    "bonding_breaks = {}\n",
    "average_bond_breaks = []\n",
    "\n",
    "for d in linker_bond_dic:\n",
    "    \n",
    "    tmp = []\n",
    "    for i, new_bonds in enumerate(linker_bond_dic[d]):\n",
    "        if i == 0:\n",
    "            ref = new_bonds\n",
    "            \n",
    "        \n",
    "        tmp.append( (ref != new_bonds).sum() )\n",
    "        \n",
    "    bonding_breaks[d] = tmp\n",
    "    average_bond_breaks.append(np.mean(tmp))\n",
    "    \n",
    "average_bond_breaks = np.array(average_bond_breaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "b979882f-1d88-4f88-83a7-da1712a88beb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[43, 51, 79, 81]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "varying_sections"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
